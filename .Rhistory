grouped_terms <-list(
species=selected_terms[c(1,3,4,5,15,17,21,22,31,39,41,53)],
conplan=selected_terms[c(7,8,9,10,11,16,18,19,20,23,26,27,28,29,30,33,40,43,45,46,47,55,60,61,62)],
sdm=selected_terms[c(12,13,14,22,24,25,32,34,35,36,37,38,42,44,48,49,50,51,52,54,56,57,58,59)]
)
grouped_terms #removed Africa, China
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE,
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
#copy and paste text into new search in search engine
#export citations like before
new_results <- import_results(file="pubmed-group-set.nbib")
nrow(new_results)
#check if all of naive search results were still included
naive_results %>%
mutate(in_new_results=title %in% new_results[, "title"]) ->
naive_results
naive_results %>%
filter(!in_new_results) %>%
select(title, keywords)
##Finally, check if our search still contains important articles that we know
important_titles <- c(
"Species distribution models for conservation planning in fire-prone landscapes",
"What are the roles of species distribution models in conservation planning?",
"Spatially explicit species distribution models: A missed opportunity in conservation planning?"
)
data.frame(check_recall(important_titles, new_results[, "title"]))
grouped_terms
grouped_terms <-list(
# species=selected_terms[c(1,3,4,5,15,17,21,22,31,39,41,53)],
conplan=selected_terms[c(7,8,9,10,11,16,18,19,20,23,26,27,28,29,30,33,40,43,45,46,47,55,60,61,62)],
sdm=selected_terms[c(12,13,14,22,24,25,32,34,35,36,37,38,42,44,48,49,50,51,52,54,56,57,58,59)]
)
grouped_terms #removed Africa, China
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE,
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
library(dplyr)
library(ggplot2)
library(ggraph)
library(igraph)
library(readr)
library(litsearchr)
packageVersion("litsearchr")
setwd("C:/Users/cmarti26/Documents/2. Projects/1.1 Review/Analysis")
#load results from search
naive_results <- import_results(file="wos1.ris")
nrow(naive_results)
colnames(naive_results)
naive_results1 <- import_results(file="wos1.ris")
naive_results2 <- import_results(file="wos2.ris")
naive_results3 <- import_results(file="wos3.ris")
nrow(naive_results1)
nrow(naive_results2)
nrow(naive_results3)
colnames(naive_results1)
colnames(naive_results2)
colnames(naive_results3)
naive_results[1, "title"]
View(naive_results1)
res <- import_results(file="res.nbib")
colnames(res)
View(res)
rm(res)
rm(naive_results)
#need to make all columns in same order and then bind 1, 2, 3 together
#id, title, abstract, keywords, methods, type, authors, affiliation, source, year,
#volume, issue, startpage, endpage, doi, language, database
naive_results1 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
View(naive_results1)
#need to make all columns in same order and then bind 1, 2, 3 together
#id, title, abstract, keywords, methods, type, authors, affiliation, source, year,
#volume, issue, startpage, endpage, doi, language, database
naive_results1 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
#need to make all columns in same order and then bind 1, 2, 3 together
#id, title, abstract, keywords, methods, type, authors, affiliation, source, year,
#volume, issue, startpage, endpage, doi, language, database
naive_results1 <- naive_results1 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
naive_results2 <- naive_results2 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
naive_results3 <- naive_results3 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
View(naive_results1)
View(naive_results2)
View(naive_results3)
naive_results <- cbind(naive_results1, naive_results2, naive_results3)
naive_results <- rbind(naive_results1, naive_results2, naive_results3)
View(naive_results)
naive_results1[1, "title"] #check title of first result
naive_results1[1001, "title"] #check title of first result
naive_results[1001, "title"] #check title of first result
naive_results[2917, "title"] #check title of a result
#getting potential search terms
naive_results[1, "keywords"] #keywords from first article
sum(is.na(naive_results[, "keywords"])) #number of articles missing keywords
#extract keywords from author-tagged keywords
keywords <- extract_terms(keywords=naive_results[, "keywords"],
method="tagged",
min_freq=3, #keywords appear at least three times
min_n=1, #keywords at least 1 word long
max_n=3) #keywords up to 3 words long
keywords
#extract keywords from author-tagged keywords
keywords <- extract_terms(keywords=naive_results[, "keywords"],
method="tagged",
min_freq=5, #keywords appear at least three times
min_n=1, #keywords at least 1 word long
max_n=3) #keywords up to 3 words long
keywords
#search in titles/abstracts of articles
extract_terms(text=naive_results[, "title"],
method="fakerake", #this performs Rapid Automatic Keyword Extraction (RAKE) method
min_freq=3,
min_n=1,
max_n=3)
#search in titles/abstracts of articles
extract_terms(text=naive_results[, "title"],
method="fakerake", #this performs Rapid Automatic Keyword Extraction (RAKE) method
min_freq=5,
min_n=1,
max_n=3)
clinpsy_stopwords <- read_lines("clin_psy_stopwords.txt")
#search in titles/abstracts of articles
extract_terms(text=naive_results[, "title"],
method="fakerake", #this performs Rapid Automatic Keyword Extraction (RAKE) method
min_freq=5,
min_n=1,
max_n=3)
#create a list of "stopwords", which are phrases that are general science or data analysis terms
#not related to the specific topic of the article
conplan_stopwords <- read_lines("conplan_stopwords.txt")
all_stopwords <- c(get_stopwords("English"), conplan_stopwords) #also get list of English stopwords and add to list
#extract only relevant search terms in title without stopwords
title_terms <- extract_terms(
text=naive_results[, "title"],
method="fakerake",
min_freq=5,
min_n=1,
max_n=3,
stopwords=all_stopwords
)
title_terms
#extract only relevant search terms in title without stopwords
title_terms <- extract_terms(
text=naive_results[, "title"],
method="fakerake",
min_freq=5,
min_n=2,
max_n=3,
stopwords=all_stopwords
)
title_terms
#extract only relevant search terms in title without stopwords
title_terms <- extract_terms(
text=naive_results[, "title"],
method="fakerake",
min_freq=5,
min_n=2,
max_n=4,
stopwords=all_stopwords
)
title_terms
#add search terms from article titles and keywords, removing duplicates
terms <- unique(c(keywords, title_terms))
terms
#extract keywords from author-tagged keywords
keywords <- extract_terms(keywords=naive_results[, "keywords"],
method="tagged",
min_freq=5, #keywords appear at least three times
min_n=2, #keywords at least 2 word long
max_n=3) #keywords up to 3 words long
keywords
#extract keywords from author-tagged keywords
keywords <- extract_terms(keywords=naive_results[, "keywords"],
method="tagged",
min_freq=5, #keywords appear at least three times
min_n=2, #keywords at least 2 word long
max_n=4) #keywords up to 4 words long
keywords
t
#add search terms from article titles and keywords, removing duplicates
terms <- unique(c(keywords, title_terms))
terms
packageVersion("litsearchr")
k
keywords
title_terms
#extract keywords from author-tagged keywords
keywords <- extract_terms(keywords=naive_results[, "keywords"],
method="tagged",
min_freq=10, #keywords appear at least 5 times
min_n=2, #keywords at least 2 word long
max_n=4) #keywords up to 4 words long
keywords #535 keywords
#extract search terms in title without stopwords
title_terms <- extract_terms(
text=naive_results[, "title"],
method="fakerake", #this performs Rapid Automatic Keyword Extraction (RAKE) method
min_freq=10,
min_n=2,
max_n=4,
stopwords=all_stopwords
)
title_terms #200 title terms
#add search terms from article titles and keywords, removing duplicates
terms <- unique(c(keywords, title_terms))
## 4.0 NETWORK ANALYSIS
docs <- paste(naive_results[, "title"], naive_results[, "abstract"])
docs[1] #check that it did this correctly
#create matrix to record which terms appear in which articles
dfm <- create_dfm(elements=docs, features=terms)
dfm[1:3, 1:4] #matrix with 0 and 1 for absence/presence of search terms within each document
dfm[1:20, 1:4] #matrix with 0 and 1 for absence/presence of search terms within each document
#turn matrix into network of linked search terms
g <- create_network(dfm, min_studies=3)
#plot network
ggraph(g, layout="stress") +
coord_fixed() +
expand_limits(x=c(-3, 3)) +
geom_edge_link(aes(alpha=weight)) +
geom_node_point(shape="circle filled", fill="white") +
geom_node_text(aes(label=name), color="white", hjust="outward", check_overlap=TRUE) +
guides(edge_alpha=FALSE)
## 5.0 PRUNE SEARCH TERMS
#rank search terms by importance
strengths <- strength(g)
data.frame(term=names(strengths), strength=strengths, row.names=NULL) |>
mutate(rank=rank(strength, ties.method="min")) |>
arrange(strength) ->
term_strengths
term_strengths
#visualize first
cutoff_fig <- ggplot(term_strengths, aes(x=rank, y=strength, label=term)) +
geom_line() +
geom_point() +
geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)
cutoff_fig #terms in ascending order of strength (labeled arbitrarily)
#first strategy: cumulatively
##retain certain proportion of total strength in network of search terms
cutoff_cum <- find_cutoff(g, method="cumulative", percent=0.8)
cutoff_cum #cutoff strength value
##visualize cutoff
cutoff_fig +
geom_hline(yintercept=cutoff_cum, linetype="dashed")
##applies cutoff and prunes terms w low strength
get_keywords(reduce_graph(g, cutoff_cum))
#second strategy: changepoints
##points along ranking where strength increases much more than previous
cutoff_change <- find_cutoff(g, method="changepoint", knot_num=3)
cutoff_change #suggests cutoffs
##visualize
cutoff_fig +
geom_hline(yintercept=cutoff_change, linetype="dashed")
##choose a cutoff value
g_redux <- reduce_graph(g, cutoff_change[2]) #specify which cutoff value
selected_terms <- get_keywords(g_redux)
selected_terms
##choose a cutoff value
g_redux <- reduce_graph(g, cutoff_change[1]) #specify which cutoff value
selected_terms <- get_keywords(g_redux)
selected_terms
##choose a cutoff value
g_redux <- reduce_graph(g, cutoff_change[2]) #specify which cutoff value
selected_terms <- get_keywords(g_redux)
selected_terms
## 6.0 GROUPING SEARCH TERMS
#create groupings by hand as a revised list for new search query
grouped_terms <-list(
conplan = selected_terms[c(1, 2, 3, 7, 8)],
sdm = selected_terms[c(4, 5, 6, 9, 10, 11, 12)]
)
grouped_terms
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE,
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
#first strategy: cumulatively
##retain certain proportion of total strength in network of search terms
cutoff_cum <- find_cutoff(g, method="cumulative", percent=0.9)
cutoff_cum #cutoff strength value
##visualize cutoff
cutoff_fig +
geom_hline(yintercept=cutoff_cum, linetype="dashed")
#visualize first
cutoff_fig <- ggplot(term_strengths, aes(x=rank, y=strength, label=term)) +
geom_line() +
geom_point() +
geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)
cutoff_fig #terms in ascending order of strength (labeled arbitrarily)
##visualize cutoff
cutoff_fig +
geom_hline(yintercept=cutoff_cum, linetype="dashed")
#first strategy: cumulatively
##retain certain proportion of total strength in network of search terms
cutoff_cum <- find_cutoff(g, method="cumulative", percent=0.95)
cutoff_cum #cutoff strength value
##visualize cutoff
cutoff_fig +
geom_hline(yintercept=cutoff_cum, linetype="dashed")
#first strategy: cumulatively
##retain certain proportion of total strength in network of search terms
cutoff_cum <- find_cutoff(g, method="cumulative", percent=0.5)
cutoff_cum #cutoff strength value
##visualize cutoff
cutoff_fig +
geom_hline(yintercept=cutoff_cum, linetype="dashed")
##applies cutoff and prunes terms w low strength
get_keywords(reduce_graph(g, cutoff_cum)) #81 terms left
cat(read_file("search-inEnglish.txt"))
grouped_terms
## 7.0 SEARCH WITH NEW TERMS
#search WoS and export results
#(( \"conservation plan\" OR \"protected area\") AND (\"distribution models\" OR \"environmental variables\" OR \"habitat suitability\" OR \"species distribution\" OR \"suitable habitat\"))
#857 results, November 21, 2022
new_results <- import_results(file="wos_newterms.ris")
nrow(new_results)
#check if all of naive search results were still included
naive_results %>%
mutate(in_new_results=title %in% new_results[, "title"]) ->
naive_results
naive_results %>%
filter(!in_new_results) %>%
select(title, keywords)
new_results <- new_results |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
#Finally, check if our search still contains important articles that we know
important_titles <- c(
"Species distribution models for conservation planning in fire-prone landscapes",
"What are the roles of species distribution models in conservation planning?",
"Spatially explicit species distribution models: A missed opportunity in conservation planning?"
)
data.frame(check_recall(important_titles, new_results[, "title"]))
cat(read_file("search-inEnglish.txt"))
## 6.0 GROUPING SEARCH TERMS
#create groupings by hand as a revised list for new search query
grouped_terms <-list(
conplan = selected_terms[c(2, 3, 7, 8)], #do not include climate change
sdm = selected_terms[c(4, 5, 6, 9, 10, 11, 12)]
)
grouped_terms
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=TRUE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
cat(read_file("search-inEnglish.txt"))
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=TRUE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="right", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
cat(read_file("search-inEnglish.txt"))
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=TRUE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
cat(read_file("search-inEnglish.txt"))
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
cat(read_file("search-inEnglish.txt"))
#Finally, check if our search still contains important articles that we know
important_titles <- c(
"Species distribution models for conservation planning in fire-prone landscapes",
"What are the roles of species distribution models in conservation planning?",
"Spatially explicit species distribution models: A missed opportunity in conservation planning?"
)
data.frame(check_recall(important_titles, new_results[, "title"]))
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="right", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="right", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
#write a new search with these grouped terms
write_search(
grouped_terms,
languages="English", #can choose multiple languages to translate into
exactphrase=TRUE, #match terms exactly rather than two separate words
stemming=FALSE, #TRUE takes all variants of words (e.g., behaviour, behavioural)
closure="left", #partial matches matched at left end of word
writesearch=TRUE #write text file
)
cat(read_file("search-inEnglish.txt"))
new_results1 <- import_results(file = "wos_newterms1.ris")
new_results2 <- import_results(file = "wos_newterms2.ris")
new_results1 <- new_results1 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
new_results2 <- new_results2 |> select(accession_zr, title, abstract, keywords, author, address, source, year,
volume, issue, start_page, end_page, doi, language, source_type)
new_results <- rbind(new_results1, new_results2)
nrow(new_results)
#check if all of naive search results were still included
naive_results |>
mutate(in_new_results=title %in% new_results[, "title"]) ->
naive_results
naive_results |>
filter(!in_new_results) |>
select(title, keywords)
#Finally, check if our search still contains important articles that we know
important_titles <- c(
"Species distribution models for conservation planning in fire-prone landscapes",
"What are the roles of species distribution models in conservation planning?",
"Spatially explicit species distribution models: A missed opportunity in conservation planning?"
)
data.frame(check_recall(important_titles, new_results[, "title"]))
#check if all of naive search results were still included
naive_results |>
mutate(in_new_results=title %in% new_results1[, "title"]) ->
naive_results
naive_results |>
filter(!in_new_results) |>
select(title, keywords)
#Finally, check if our search still contains important articles that we know
important_titles <- c(
"Species distribution models for conservation planning in fire-prone landscapes",
"What are the roles of species distribution models in conservation planning?",
"Spatially explicit species distribution models: A missed opportunity in conservation planning?"
)
data.frame(check_recall(important_titles, new_results1[, "title"]))
View(new_results)
hist(new_results$year)
hist(year ~ . , data = new_results)
as.numeric(new_results$year)
hist(year ~ . , data = new_results)
plot(year ~ . , data = new_results)
hist(new_results$year)
hist(as.numeric(new_results$year))
write.csv(new_results, col.names = TRUE)
write.csv(new_results, file = "new_results.csv")
View(new_results1)
biblio <- read.csv("C:/Users/cmarti26/Documents/2. Projects/1.1 Review/Analysis/new_results.csv")
biblio
View(biblio)
hist(year)
hist(biblio$year)
biblio2012 <- subset(biblio, year > 2012)
rm(biblio2012)
install.packages("rmarkdown")
library(rmarkdown)
setwd("~/Documents/University/PhD/Supervision & Teaching/Elea/u2018_clc2012_v2020_20u1_raster100m/DATA")
setwd("~/Documents/University/PhD/Supervision & Teaching/Elea/u2018_clc2012_v2020_20u1_raster100m/DATA")
setwd("C:/Users/cmarti26/Documents/4. Teaching & Supervision/firststep2022/landcover/DATA/")
#open raster
lc2012 <- raster(paste0("U2018_CLC2012_V2020_20u1.tif"))
#CORINE LC 2012
library("raster")
#open raster
lc2012 <- raster(paste0("U2018_CLC2012_V2020_20u1.tif"))
#open raster
lc2012 <- raster(paste0("U2018_CLC2018_V2020_20u1.tif"))
lc2012
setwd("C:/Users/cmarti26/Documents/4. Teaching & Supervision/firststep2022/landcover/DATA")
#open raster
lc2012 <- raster(paste0("U2018_CLC2012_V2020_20u1.tif"))
#open raster
lc2012 <- raster(paste0("U2018_CLC2018_V2020_20u1.tif"))
lc2012
#open raster
lc2012 <- raster(paste0("U2018_CLC2012_V2020_20u1.tif"))
lc2012
